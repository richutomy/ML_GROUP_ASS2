{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "R",
      "language": "R",
      "name": "ir"
    },
    "language_info": {
      "codemirror_mode": "r",
      "file_extension": ".r",
      "mimetype": "text/x-r-source",
      "name": "R",
      "pygments_lexer": "r",
      "version": "3.3.1"
    },
    "colab": {
      "name": "ML_ASS2.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/richutomy/ML_GROUP_ASS2/blob/master/ML_ASS2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vx17_zzzbclj",
        "colab_type": "text"
      },
      "source": [
        "# SENTIMENTAL ANALYSIS OF TWITTER DATA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "On7nScsUbmiq",
        "colab_type": "text"
      },
      "source": [
        "##1. BUSINESS UNDERSTANDING\n",
        "\n",
        " **1.1 BUSINESS PROBLEM**\n",
        "\n",
        "Social media proves to be a medium for the users to get details more easily and also understand multiple perspectives about different products and benefits(Kang, 2018). As a result of the data  shared through social media can affect a potential consumer’s decision to make a purchase. Social media promotes interaction, coordination as well as sharing of content (Palmer & Koenig-Lewis, 2009). This can be through blogs, social network platforms as well as review websites. Such sites can also be used to promote brands like Pinterest. The information that is shared in the Internet about different views of customers about products can be utilised by firms to evaluate the general feedback of the customers to further improve their offerings and more over ensure a higher customer satisfaction.\n",
        "\n",
        "In this project, we aim to utilise the data available on social media to understand the general opinion of users about a product that would assist the vendor to further iterate their product. As part of this assignment, sentimental analysis would be used to understand the general trends in social media. Sentimental analysis can be defined as the procedure of computationally recognising and classifying feedbacks that is usually elaborated through text in order to identify the attitude of the writer as either 'positive', 'negative' or 'neutral'.  Thus the business problem statement of this project is as follows:\n",
        "Analysing the overall feedback of consumers with regards to the launch of the latest Apple iPhone 11 through Sentimental Analysis utilising Twitter Data. The data from Twitter is considered because the information is easily accessible when compared to other social networking sites such as Facebook and Instagram. In this project, tweets containing information @apple , #Iphone11 and so on. The mentioned business problem statement can be further divided into sub-sections as follows:\n",
        "\n",
        "\n",
        "\n",
        "*   Gathering Tweets through Twitter application\n",
        "*   Gathering Tweets through Twitter application\n",
        "*   Conducting Sentimental Analysis\n",
        "*   Executing Machine learning Algorithms to classify sentiments as positive, negative or neutral.\n",
        "\n",
        "\n",
        "Through this project, a firm can be assisted in comprehending users’ emotions regarding the firm’s brand, products and services. In addition, stake holders of the business can understand how an action can bring either a positive or negative impact on the business. This would in turn enhance the customer satisfaction and customer service.\n",
        "\n",
        "\n",
        "**1.2 DATA MINING PROBLEMS**\n",
        "\n",
        "The data mining problem can be considered as two stages:\n",
        "\n",
        "1.  Data cleaning/conditioning stage:  In this stage, the raw data from twitter needs to be cleaned and transformed into cleaned dataset that facilitates computation of predicting factors\n",
        "\n",
        "    This stage consists of the following steps:\n",
        "\n",
        "\n",
        "*   Data gathering from twitter\n",
        "*   Data pre-processing\n",
        "\n",
        "\n",
        "2.  Prediction & Analysis stage:  In this stage, a prediction model needs to be created that will assist predictions of outcomes dependent on a new variety of factors that were not included in the primary datasets.\n",
        "\n",
        "\n",
        "     This stage consists of the following steps:\n",
        "\n",
        "\n",
        "*   Prediction Model building and data analysis\n",
        "*   Correlation and Prediction Analysis\n",
        "\n",
        "\n",
        "\n",
        "The framework (Figure 1) utilised in this project is along the lines of Social Media Mining (SMM), Big Data, Sentimental Analysis, Data Mining, and Machine Learning methodologies that is utilised to classify and evaluate data from tweets.\n",
        "![alt text](https://drive.google.com/uc?id=1shcwYkNkV3n6Lpu60rsnOxoblFnboEQb)\n",
        "                **FIGURE 1: DATA MINING PROBLEM FRAMEWORK**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "The steps required for the Data Mining problem can thus be briefed as per the below figure:\n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=1qnOjmPHFh8BsqC36_k1Fai5AlyCZ9iMz)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csj7swWsjqqV",
        "colab_type": "text"
      },
      "source": [
        "#2. PROJECT EXECUTION:\n",
        "![alt text](https://drive.google.com/uc?id=1FwDKR8xpdg5e9nrhphD9I-EuT1yrIn6c)\n",
        "\n",
        "The above figure (Figure 3) visualises the steps required in executing this process. These are\n",
        "\n",
        "\n",
        "\n",
        "1.   Reading Files\n",
        "2.   Building Corpus\n",
        "3.   Cleaning Text\n",
        "4.   Building Term Document Matrix\n",
        "5.   Plotting data to visualize them\n",
        "6.  Data Analysis List item\n",
        "\n",
        "\n",
        "These steps are further elaborated in the following sections.\n",
        "\n",
        "\n",
        "\n",
        "**2.1 PROGRAMMING LANGUAGE – R:**\n",
        "\n",
        "R is one of the programming language widely used by data miners and statisticians to generate computable models for the analysis of data. It provides software environment for statistical computations, categorizations, and grouping, graphic techniques and so on. Different packages are present in this language to assists in these tasks. The following packages are taken implement this project.\n",
        "\n",
        "1)\ttwitteR: API that provides the web interface for twitter\n",
        "\n",
        "2)\tROAuth: R interface of OAuth. Allows the authentication of the server of user’s choice through OAuth.\n",
        "\n",
        "3)\tTm: The package used for text mining\n",
        "\n",
        "4)\tTmap: Geographical maps used to visualize arrangement of spatial data. The package offers an approach that is flexible, easy-to-handle and has layered basis for the creation of thematic maps.\n",
        "\n",
        "5)\tTidyverse: Group of R packages designed for data scientists\n",
        "\n",
        "6)\tTidytext: Contains functions which permits the conversion of the text ‘to and from’ tidy formats and to switch between tools and existing packages.\n",
        "\n",
        "7)\tWordcloud: Allows creation of beautiful world clouds,provides visuals showing the similarity and dissimilarity between documents and also helps to avoid the problem of over-plotting in scatter plots using text.\n",
        "\n",
        "8)\tSyuzhet: Helps in the extraction of sentiments and ‘sentiment-derived’ plots from text using different sentiment dictionaries.\n",
        "\n",
        "9)\tsnowballC:  Use ‘Porter’s stemming algorithm’ for subsiding words to a common root to facilitate the comparison of the vocabulary. The package provides interface to C ‘libstemmer’ library.\n",
        "\n",
        "10)\tstringr:  Provides function that makes working with strings lot easier\n",
        "\n",
        "11)\tdplyr: Makes works related with data more easier by constraining options, providing ‘simple verb’ functions and by  using efficient back ends.\n",
        "\n",
        "12)\tdevtools: Tool that helps in the easy development of R packages\n",
        "\n",
        "13)\tRCurl: Helps in composing common HTTP requests and offers functions to retrieve URIs and also take actions on results returned bynthe web server.\n",
        "\n",
        "14)\tigraph: Helps in the analysis of network and visualization of graph\n",
        "\n",
        "15)\trtweet: Supports the collection and organization of twitter data through the APIs of twitter\n",
        "\n",
        "16)\tRColorBrewer: Helps to create fine colour palettes\n",
        "\n",
        "17)\tRSentiment: Used for the sentiment analysis of English sentence. The sentences will be classified to different categories like 'positive', 'negative', 'neutral', 'very positive', 'very negative' based on the score assigned to each sentence.  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**2.2 TWITTER APPLICATION CREATION**\n",
        "\n",
        "\n",
        "R provides conventional text analysis using different text processing and analysis packages. For doing the text analysis, data should be collected from twitter using API of twitter. An application should be registered for accessing twitter API. Only the public information on twitter can be accessed using the applications by default. The application helps in analysis through the linkage of R console with Twitter.\n",
        "\n",
        "Below given are the steps used in the creation of Twitter application\n",
        "\n",
        "1)\tUse https://dev.twitter.com/ to login to your twitter account\n",
        "\n",
        "2)\tBrowse to the My Applications section of the website to create a new application\n",
        "\n",
        "The below snapshot gives details of the new application that has been created.\n",
        "\n",
        "On completion of creating the application, twitter will provide the following details:\n",
        "\n",
        "1)\t'Consumer API Key'\n",
        "\n",
        "2)\t'Consumer Secret API Key'\n",
        "\n",
        "3)\t'Access Token' and \n",
        "\n",
        "4)\t'Access Token Secret'\n",
        "\n",
        "\n",
        "Once these details are obtained, it is used to authenticate the Twitter object on R. For this process, both twitteR and ROAuth libraries are used with the utilisation of the function setup_twitter_oauth().\n",
        "The code for authorisation is shown in the snap shot below. \n",
        "\n",
        "Once the above code is executed, the page is redirected to the twitter API page on a web browser which requests us to authorise the application and create a distinct code. After typing in this code on the console section of the R Studio, a ‘true’ indication is returned confirming the handshake between the R environment and twitter application as complete. The next step after this is to collect twitter data (Tweets) from its timeline.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**2.3 TWITTER DATA EXTRACTION AND LOADING**\n",
        "\n",
        "Once the handshake is complete, we can now collect the recent tweets related to provide hashtags on twitter. As part of this project, 1500 tweets are collected using the #iphone11 hashtag. This gathered information would be used as the testing dataset for building prediction models. The below snapshot shows the code used for this process.\n",
        "\n",
        "\n",
        "In order to collect and save data from Twitter, a function ‘Search twitter’ is used. The format of this downloaded file is a data frame (df) that is further transformed to a .csv format. The below snapshot shows the .csv file that has been downloaded. In order to build a prediction model, a training set is also required. For this purpose, a data source Kaggle is used. A related dataset is retrieved provided in the link https://www.kaggle.com/youben/twitter-sentiment-analysis/data\n",
        "The selected dataset has the following attributes:\n",
        "-\tTweet ID\n",
        "-\tText\n",
        "-\tSentiment!\n",
        "In addition to this dataset, a data dictionary of both positive and negative words are used with the data from the dataset to classify the data based on the sentiments involved. This dictionary is also utilised with the training dataset.\n",
        "\n",
        "\n",
        "**2.4 DATA DICTIONARY**\n",
        "\n",
        "This section describes the different attributes that are present as part of the testing data set.\n",
        "![alt text](https://drive.google.com/uc?id=1RUGyYGIWJEOHNdAu6UoNvhhzdccXBuI9)\n",
        "\n",
        "**2.5 DATA TRANFORMATION**\n",
        "\n",
        "\n",
        "The data taken from the twitter API is not cleaned and is in a raw format. The data that is collected is noisy in nature and has many emotions, punctuations, URLS and blank spaces that needs to be removed. This is done through data pre-processing. In addition, all the attributes mentioned in the table above need not be taken in consideration. Only relevant attributes such as text and ID need to be taken and the rest can be deleted. The following figure illustrates the processes involved in data transformation in this project\n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=1yGmx8PZv0h-dBuhG5Wv6duH4OBqNNw_v)\n",
        "FIGURE 4: STEPS IN DATA TRANSFORMATION \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "   **2.5.1 TWEET CLEANING**\n",
        "   \n",
        "In order to clean tweets, a library of R, 'tm' is used. This package is utilised to carry out multiple data transformation processes to further clean data more efficiently and quickly.  The following techniques are used to clean the raw noisy tweets.\n",
        "\n",
        "1)\tTransforming tweets to all lower-case\n",
        "Since the operating language R is case-sensitive in nature, it should be ensured that all the data is consistent in nature and have a standardformat. For this operation, the following code is used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "2)\tFiltering out numbers, emoticons and punctuations\n",
        "The above code is also used to remove unwanted numbers, emoticons and punctuations as they wouldn’t help in giving meaning to classification.\n",
        "\n",
        "3)\tFiltering out URLs\n",
        "URLs in the tweet data is also removed from the dataset using the below code.\n",
        "\n",
        "\n",
        "4)\tFiltering out blank spaces\n",
        "Unnecessary blank spaces are removed from the data using the stripWhitespace function in the code above.\n",
        "\n",
        "\n",
        "5)\tFiltering out stop words, substitute for abbreviations as well as performing spelling corrections\n",
        "Irregular words in the data set has the ability to change the context and meaning of a data record. In order to reduce such errors, verbs, qualifiers (nonetheless, is, yet), articles (the, an, a) and conjunctions (or, and, at the same time) which are known as 'stop words' in the tm package are removed. \n",
        "\n",
        "The above plot shows the distinct words that are occuring more frequently in tweets.  As per the figure, the word iphone occurs the most in the tweets. As it is occurring multiple times, it can be deleted.\n",
        "\n",
        "\n",
        "6) Stemming and Lemmatization\n",
        "\n",
        "Stemming is defined as the process of reducing related words to its normal original form by removing the ends of the word. For example, words such as following and followed can be reduced to its original form of follow. Unfortunately, this process has the chance of creating further confusions. For instance through stemming, two words insured and insurance will be minimised to insur instead of the original tense word of insurance. In order to reduce such errors, lemmatization process is conducted as it also considers the grammatical context in which the words occur. Through this process, the selected words are considered as either adjectives, nouns, or verbs and then the root of the word (lemma) is considered.\n",
        "In order to perform these transformations in an R-environment, libraries such as twitter, SnowballC, tm and tm map are used.\n",
        "\n",
        "7) CREATING TERM-DOCUMENT MATRIX\n",
        "\n",
        "\n",
        "A 'term-document matrix' is created using a set of virtually saved texts similar to a data repository called a corpus.Every occurrences of words are represented in the corpus using records. The rows of the document represents words and the column represents words. The working of matrix is in a way that if a word is present in the document, the particular entry on the matrix that corresponds to that column and row is given a value one or else the value would be zero. If a word occurs thrice in a document, the entry value of the would be updated as three. The below figure shows a sample 'Term-Document Matrix'.\n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=1T1r32851xqLyT2ojbKc4twqnSR8dZMeh)\n",
        "\n",
        "\n",
        "**2.5.2. TWEET PARSING**\n",
        "\n",
        "This is the final step carried out as part of data transformation post tweet cleaning. The following steps are carried out for this task.\n",
        "\n",
        "\n",
        "*   COLLECTING HASHTAGS\n",
        "\n",
        "On every social network, the hashtag symbol has become a means to raise opinions and express sentiments. As a result, this syntax is used to collect our relevant data. A search for #iphone11 is carried out to gather related tweets and further analyse a general sentiment on this new product.\n",
        "\n",
        "*   FILTERING OUT WORDS THAT CHANGES CONTEXT\n",
        "\n",
        "The gathered data consists of many words such as ‘but’, ‘and’ and ‘or’ that has the tendency to change the context of a post. In order to reduce such errors, these words are filtered out for enhanced accuracy.\n",
        "\n",
        "**SENTIMENTAL ANALYSIS** \n",
        "\n",
        "Sentiment analysis is defined as the process where the collected text data is evaluated to identify the emotion and attitude of the twitter user. The results can be either positive, negative or neutral. To carry out this process, the collected data needs to transformed to useful information and then be compared with a list of words, both positive and negative that are available online. This list of words are saved onto the present working directory of the R environment. For conducting manipulation with string datatypes, we have used both plyr as well as the stringr packages from R. To evaluate the score for individual tweet, we used a sentiment function. In order to classify words in tweet data as positive or negative, each word from tweets are respectively compared with 'positive' and 'negative' word lists respectively. The below snap shot shows the code used to carry out this process.\n",
        "\n",
        "\n",
        "\n",
        "To further visualise the results of the sentiment analysis, SocioViz tool is used to visualise a polarity of resulting sentiments. As per the figure below, the number of positive feedbacks given to the product such as 'pleasant', 'excited' and 'relaxed' are more compared to the negative feedbacks. The negative comments are represented with a one and positive comments are represented as a zero. The area under the positive comments appear to be denser than the negative comments. As a result it can be understand that the general sentiment towards the product is positive.\n",
        "\n",
        "**SENTIMENT ANALYSIS – WORD CLOUD**\n",
        "\n",
        "In this section, the results of sentiment analysis is visualised through a process called Word Cloud. It shows the  visualisation of the  words with highest frequency of occurrences. To visualise this a package called word cloud is used on R Studio. The code used for this process is shown in the snapshot below.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYaLo8Y3kvbN",
        "colab_type": "text"
      },
      "source": [
        "# 3. DATA MODELLING\n",
        "\n",
        "The text gathered through 'natural language processing' (NLP) is never is a decipherable format by the computer. As a result selecting features is very important. Unwanted elements present in gathered data such as emoticons, URLs as well as stop words cannot be provided to the ML algorithm directly and thus data pre-processing needs to be done. After this process, the tweets is transformed to a list of tokens.\n",
        "\n",
        "This transformation is made through n-gram modelling where n can be any number starting from 1. An example for a unigram can be (‘I’, ’love’, ’the’, ’way’, ’the’, ’camera’). Similarly an example of an bigram can be (‘I love’, ’the way’, ’the camera’). \n",
        "\n",
        "Post transforming the words into a list of words through n-gram programming, the necessary features are selected.\n",
        "\n",
        "**DATA CLASSIFICATION:**\n",
        "\n",
        "For the classification of data,classifiers like Neural networks, K-Nearest Neighbours, Random Forest as well as Naïve Bayesian are used. The accuracy of every classifier used here is compared with the each other to find the classifier that performs sentiment analysis best with the collected set of data.\n",
        "\n",
        "\n",
        "\n",
        "1.   KNN CLASSIFIER:\n",
        "\n",
        "KNN classifier is a great tool for the purpose of sentiment analysis. The advantage is that, KNN doesn’t make any assumptions regarding the fundamentaldata. KNN is non-linear and non-parametric in nature. All the calculations are carried out post calculations.  As per the algorithm, the dataset is kept as is while training and each class is provided with respective tweets which is illustrated by majority label of its nearest neighbours present in the training dataset.\n",
        "\n",
        "2.   RANDOM FOREST CLASSIFIER:\n",
        "\n",
        "This algorithm merges more than one decision trees for generating outputs. To build classifiers, the algorithm make use of the  bagging techniques. This methodology uses the technique of voting where the greatervotes is returned to the class. If there is a dataset D with T attributes and N number of tweets, the random forest classifier is built as follows:\n",
        "Multiple decision trees d are generated which fall under sub-levels of D. For respective decision tree d, a random group of attributes is taken as candidate attributes. K number of decisions trees are created in this manner and aggregated to form a random forest classifier. The tweets are classified based on the maximum votes obtained for respective classes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faFb59msmcw8",
        "colab_type": "text"
      },
      "source": [
        "**CODE OF EXECUTION**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oFbZfk0nMpg",
        "colab_type": "text"
      },
      "source": [
        "Installing packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAdllslYmjQO",
        "colab_type": "code",
        "outputId": "afa4a8f6-69a1-4906-b324-e1a7a3137bbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "install.packages(c(\"ROAuth\",\"plyr\",\"stringr\",\"ggplot2\",\"tm\",\"twitteR\",\"ROAuth\",\"RCurl\",\"wordcloud\"), dependencies = TRUE)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Installing packages into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "Warning message:\n",
            "“dependencies ‘Rcampdf’, ‘Rgraphviz’, ‘tm.lexicon.GeneralInquirer’, ‘Rcompression’ are not available”also installing the dependencies ‘checkmate’, ‘zoo’, ‘e1071’, ‘fontBitstreamVera’, ‘fontLiberation’, ‘httpuv’, ‘xtable’, ‘sourcetools’, ‘later’, ‘promises’, ‘rex’, ‘Formula’, ‘latticeExtra’, ‘acepack’, ‘gridExtra’, ‘data.table’, ‘htmlTable’, ‘viridis’, ‘sp’, ‘mvtnorm’, ‘TH.data’, ‘sandwich’, ‘SparseM’, ‘MatrixModels’, ‘classInt’, ‘units’, ‘diffobj’, ‘fontquiver’, ‘freetypeharfbuzz’, ‘shiny’, ‘qpdf’, ‘bit’, ‘blob’, ‘abind’, ‘foreach’, ‘doParallel’, ‘itertools’, ‘iterators’, ‘covr’, ‘htmlwidgets’, ‘ggplot2movies’, ‘hexbin’, ‘Hmisc’, ‘mapproj’, ‘maps’, ‘maptools’, ‘multcomp’, ‘profvis’, ‘quantreg’, ‘rgeos’, ‘sf’, ‘vdiffr’, ‘NLP’, ‘slam’, ‘antiword’, ‘filehash’, ‘pdftools’, ‘Rpoppler’, ‘SnowballC’, ‘bit64’, ‘rjson’, ‘RSQLite’, ‘RMySQL’, ‘bitops’, ‘XML’\n",
            "\n",
            "Warning message in install.packages(c(\"ROAuth\", \"plyr\", \"stringr\", \"ggplot2\", \"tm\", :\n",
            "“installation of package ‘units’ had non-zero exit status”Warning message in install.packages(c(\"ROAuth\", \"plyr\", \"stringr\", \"ggplot2\", \"tm\", :\n",
            "“installation of package ‘Rpoppler’ had non-zero exit status”Warning message in install.packages(c(\"ROAuth\", \"plyr\", \"stringr\", \"ggplot2\", \"tm\", :\n",
            "“installation of package ‘RMySQL’ had non-zero exit status”Warning message in install.packages(c(\"ROAuth\", \"plyr\", \"stringr\", \"ggplot2\", \"tm\", :\n",
            "“installation of package ‘rgeos’ had non-zero exit status”Warning message in install.packages(c(\"ROAuth\", \"plyr\", \"stringr\", \"ggplot2\", \"tm\", :\n",
            "“installation of package ‘pdftools’ had non-zero exit status”Warning message in install.packages(c(\"ROAuth\", \"plyr\", \"stringr\", \"ggplot2\", \"tm\", :\n",
            "“installation of package ‘sf’ had non-zero exit status”"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybZIPVIZrB3O",
        "colab_type": "text"
      },
      "source": [
        "importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6iQDwGGrKzM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "library(twitteR)\n",
        "library(ROAuth)\n",
        "library(plyr)\n",
        "library(stringr)\n",
        "library(ggplot2)\n",
        "library(RCurl)\n",
        "library(RColorBrewer)\n",
        "library(tm)\n",
        "library(wordcloud)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sP73WLMrriDw",
        "colab_type": "text"
      },
      "source": [
        "Connecting to twitter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1517Eke9rnK4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "f308a0f8-739b-4603-b951-c77c01c89e9c"
      },
      "source": [
        "options(RCurlOptions = list(cainfo = system.file(\"CurlSSL\", \"cacert.pem\", package = \"RCurl\")))\n",
        "\n",
        "\n",
        "#save these credentials and register\n",
        "\n",
        "download.file(url= \"http://curl.haxx.se/ca/cacert.pem\", destfile= \"cacert.pem\")\n",
        "credentials <- OAuthFactory$new(consumerKey='tpeVWtDW1z8lpX7OsJ5UOQFTn',\n",
        "                                consumerSecret='4TD4aCVV0xQR4b4NjkaFVd4JzIkuRHAMnm1KmDyENooaiz651u',\n",
        "                                requestURL='https://api.twitter.com/oauth/request_token',\n",
        "                                accessURL='https://api.twitter.com/oauth/access_token',\n",
        "                                authURL='https://api.twitter.com/oauth/authorize')\n",
        "\n",
        "credentials$handshake(cainfo=\"cacert.pem\")\n",
        "\n",
        "save(credentials, file=\"twitter authentication.Rdata\")\n",
        "\n",
        "load(\"twitter authentication.Rdata\")\n",
        "\n",
        "setup_twitter_oauth(credentials$consumerKey, credentials$consumerSecret, \n",
        "                    credentials$oauthKey, credentials$oauthSecret)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "To enable the connection, please direct your web browser to: \n",
            "https://api.twitter.com/oauth/authorize?oauth_token=OzaI-gAAAAABAHzBAAABbWfBhXw\n",
            "When complete, record the PIN given to you and provide it here: 8205525\n",
            "[1] \"Using direct authentication\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtPPEOpAxHF9",
        "colab_type": "text"
      },
      "source": [
        "Searching for tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TwBZkMgxMRu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "iphone11.list <- searchTwitter('#IPhone11', n=2000 , lang =\"en\")\n",
        "iphone11.df <- twListToDF(iphone11.list)\n",
        "write.csv(iphone11.df, file='iphone11.csv', row.names =F)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}